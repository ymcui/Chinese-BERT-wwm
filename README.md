## ä¸­æ–‡å…¨è¯è¦†ç›–BERT
ä¸ºäº†è¿›ä¸€æ­¥ä¿ƒè¿›ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†çš„ç ”ç©¶å‘å±•ï¼Œæˆ‘ä»¬æä¾›äº†ä¸­æ–‡å…¨è¯è¦†ç›–ï¼ˆWhole Word Maskingï¼‰BERTçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚
åŒæ—¶åœ¨æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šä¸­è¯¦ç»†å¯¹æ¯”äº†å½“ä»Šæµè¡Œçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼š[BERT](https://github.com/google-research/bert)ã€[ERNIE](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE)ã€[BERT-wwm](https://github.com/ymcui/Chinese-BERT-wwm)

**For English description, please read our technical report on arXiv: https://arxiv.org/abs/1906.08101**

**æ›´å¤šç»†èŠ‚è¯·å‚è€ƒæˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šï¼šhttps://arxiv.org/abs/1906.08101**

æœ¬é¡¹ç›®åŸºäºè°·æ­Œå®˜æ–¹çš„BERTï¼šhttps://github.com/google-research/bert


## å†…å®¹
| ç« èŠ‚ | æè¿° |
|-|-|
| [ç®€ä»‹](#ç®€ä»‹) | ä»‹ç»BERT-wwm |
| [ä¸­æ–‡æ¨¡å‹ä¸‹è½½](#ä¸­æ–‡æ¨¡å‹ä¸‹è½½) | æä¾›äº†BERT-wwmçš„ä¸‹è½½åœ°å€ |
| [ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ](#ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ) | åˆ—ä¸¾äº†éƒ¨åˆ†ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ |
| [è‹±æ–‡æ¨¡å‹ä¸‹è½½](#è‹±æ–‡æ¨¡å‹ä¸‹è½½) | è°·æ­Œå®˜æ–¹çš„è‹±æ–‡BERT-wwmä¸‹è½½åœ°å€ |
| [å¼•ç”¨](#å¼•ç”¨) | æœ¬ç›®å½•çš„æŠ€æœ¯æŠ¥å‘Š |

## ç®€ä»‹
**Whole Word Masking (wwm)**ï¼Œæš‚ä¸”ç¿»è¯‘ä¸º`å…¨è¯Mask`ï¼Œæ˜¯è°·æ­Œåœ¨2019å¹´5æœˆ31æ—¥å‘å¸ƒçš„ä¸€é¡¹BERTçš„å‡çº§ç‰ˆæœ¬ï¼Œä¸»è¦æ›´æ”¹äº†åŸé¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ ·æœ¬ç”Ÿæˆç­–ç•¥ã€‚ç®€å•æ¥è¯´ï¼ŒåŸæœ‰åŸºäºWordPieceçš„åˆ†è¯æ–¹å¼ä¼šæŠŠä¸€ä¸ªå®Œæ•´çš„è¯åˆ‡åˆ†æˆè‹¥å¹²ä¸ªè¯ç¼€ï¼Œåœ¨ç”Ÿæˆè®­ç»ƒæ ·æœ¬æ—¶ï¼Œè¿™äº›è¢«åˆ†å¼€çš„è¯ç¼€ä¼šéšæœºè¢«`[MASK]`æ›¿æ¢ã€‚åœ¨`å…¨è¯Mask`ä¸­ï¼Œå¦‚æœä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceè¢«`[MASK]`æ›¿æ¢ï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«`[MASK]`æ›¿æ¢ï¼Œå³`å…¨è¯Mask`ã€‚

åŒç†ï¼Œç”±äºè°·æ­Œå®˜æ–¹å‘å¸ƒçš„`BERT-base , Chinese`ä¸­ï¼Œä¸­æ–‡æ˜¯ä»¥**å­—**ä¸ºç²’åº¦è¿›è¡Œåˆ‡åˆ†ï¼Œæ²¡æœ‰è€ƒè™‘åˆ°ä¼ ç»ŸNLPä¸­çš„ä¸­æ–‡åˆ†è¯ï¼ˆCWSï¼‰ã€‚æˆ‘ä»¬å°†å…¨è¯Maskçš„æ–¹æ³•åº”ç”¨åœ¨äº†ä¸­æ–‡ä¸­ï¼Œä½¿ç”¨äº†ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆåŒ…æ‹¬ç®€ä½“å’Œç¹ä½“ï¼‰è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨äº†[å“ˆå·¥å¤§LTP](http://ltp.ai)ä½œä¸ºåˆ†è¯å·¥å…·ï¼‰ï¼Œå³å¯¹ç»„æˆåŒä¸€ä¸ª**è¯**çš„æ±‰å­—å…¨éƒ¨è¿›è¡Œ[MASK]ã€‚

ä¸‹è¿°æ–‡æœ¬å±•ç¤ºäº†`å…¨è¯Mask`çš„ç”Ÿæˆæ ·ä¾‹ã€‚

| è¯´æ˜ | æ ·ä¾‹ |
| :------- | :--------- |
| åŸå§‹æ–‡æœ¬ | ä½¿ç”¨è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„probabilityã€‚ |
| åˆ†è¯æ–‡æœ¬ | ä½¿ç”¨ è¯­è¨€ æ¨¡å‹ æ¥ é¢„æµ‹ ä¸‹ ä¸€ä¸ª è¯ çš„ probability ã€‚ |
| åŸå§‹Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] å‹ æ¥ [MASK] æµ‹ ä¸‹ ä¸€ ä¸ª è¯ çš„ pro [MASK] ##lity ã€‚ |
| å…¨è¯Maskè¾“å…¥ | ä½¿ ç”¨ è¯­ è¨€ [MASK] [MASK] æ¥ [MASK] [MASK] ä¸‹ ä¸€ ä¸ª è¯ çš„ [MASK] [MASK] [MASK] ã€‚ |


## ä¸­æ–‡æ¨¡å‹ä¸‹è½½
*   **`BERT-base, Chinese (Whole Word Masking)`**:
    12-layer, 768-hidden, 12-heads, 110M parameters

#### TensorFlowç‰ˆæœ¬ï¼ˆ1.12ã€1.13ã€1.14æµ‹è¯•é€šè¿‡ï¼‰
- Google: [download_link_for_google_storage](#)
- ç™¾åº¦äº‘: [download_link_for_baidu_pan](#)

#### PyTorchç‰ˆæœ¬ï¼ˆè¯·ä½¿ç”¨ğŸ¤— çš„[PyTorch-BERT](https://github.com/huggingface/pytorch-pretrained-BERT) > 0.6ï¼Œå…¶ä»–ç‰ˆæœ¬è¯·è‡ªè¡Œè½¬æ¢ï¼‰
- Google: [download_link_for_google_storage](#)
- ç™¾åº¦äº‘: [download_link_for_baidu_pan](#)

ä¸­å›½å¤§é™†å¢ƒå†…å»ºè®®ä½¿ç”¨ç™¾åº¦äº‘ä¸‹è½½ç‚¹ï¼Œå¢ƒå¤–ç”¨æˆ·å»ºè®®ä½¿ç”¨è°·æ­Œä¸‹è½½ç‚¹ï¼Œæ–‡ä»¶å¤§å°çº¦**400M**ã€‚
ä»¥TensorFlowç‰ˆæœ¬ä¸ºä¾‹ï¼Œä¸‹è½½å®Œæ¯•åå¯¹zipæ–‡ä»¶è¿›è¡Œè§£å‹å¾—åˆ°ï¼š
```
chinese_wwm_L-12_H-768_A-12.zip
    |- bert_model.ckpt      # æ¨¡å‹æƒé‡
    |- bert_model.meta      # æ¨¡å‹metaä¿¡æ¯
    |- bert_model.index     # æ¨¡å‹indexä¿¡æ¯
    |- bert_config.json     # æ¨¡å‹å‚æ•°
    |- vocab.txt            # è¯è¡¨
```
å…¶ä¸­`bert_config.json`å’Œ`vocab.txt`ä¸è°·æ­ŒåŸç‰ˆ`**BERT-base, Chinese**`å®Œå…¨ä¸€è‡´ã€‚


## ä¸­æ–‡åŸºçº¿ç³»ç»Ÿæ•ˆæœ
ä¸ºäº†å¯¹æ¯”åŸºçº¿æ•ˆæœï¼Œæˆ‘ä»¬åœ¨ä»¥ä¸‹å‡ ä¸ªä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬`å¥å­çº§`å’Œ`ç¯‡ç« çº§`ä»»åŠ¡ã€‚
**ä¸‹é¢ä»…åˆ—ä¸¾éƒ¨åˆ†ç»“æœï¼Œå®Œæ•´ç»“æœè¯·æŸ¥çœ‹æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/1906.08101)ã€‚**

- [**CMRC 2018**ï¼šç¯‡ç« æŠ½å–å‹é˜…è¯»ç†è§£ï¼ˆç®€ä½“ä¸­æ–‡ï¼‰](https://github.com/ymcui/cmrc2018)
- [**DRCD**ï¼šç¯‡ç« æŠ½å–å‹é˜…è¯»ç†è§£ï¼ˆç¹ä½“ä¸­æ–‡ï¼‰](https://github.com/DRCSolutionService/DRCD)
- [**NER**ï¼šä¸­æ–‡å‘½åå®ä½“æŠ½å–](http://sighan.cs.uchicago.edu/bakeoff2006/)
- [**THUCNews**ï¼šç¯‡ç« çº§æ–‡æœ¬åˆ†ç±»](http://thuctc.thunlp.org)

**æ³¨æ„ï¼šä¸ºäº†ä¿è¯ç»“æœçš„å¯é æ€§ï¼Œå¯¹äºåŒä¸€æ¨¡å‹ï¼Œæˆ‘ä»¬è¿è¡Œ10éï¼ˆä¸åŒéšæœºç§å­ï¼‰ï¼Œæ±‡æŠ¥æ¨¡å‹æ€§èƒ½çš„æœ€å¤§å€¼å’Œå¹³å‡å€¼ã€‚ä¸å‡ºæ„å¤–ï¼Œä½ è¿è¡Œçš„ç»“æœåº”è¯¥å¾ˆå¤§æ¦‚ç‡è½åœ¨è¿™ä¸ªåŒºé—´å†…ã€‚**


### CMRC 2018
![./pics/cmrc2018.png](https://github.com/ymcui/Chinese-BERT-wwm/raw/master/pics/cmrc2018.png)

### DRCD
![./pics/drcd.png](https://github.com/ymcui/Chinese-BERT-wwm/raw/master/pics/drcd.png)

### NER
![./pics/ner.png](https://github.com/ymcui/Chinese-BERT-wwm/raw/master/pics/ner.png)

### THUCNews
![./pics/thucnews.png](https://github.com/ymcui/Chinese-BERT-wwm/raw/master/pics/thucnews.png)


## è‹±æ–‡æ¨¡å‹ä¸‹è½½
ä¸ºäº†æ–¹ä¾¿å¤§å®¶ä¸‹è½½ï¼Œé¡ºä¾¿å¸¦ä¸Šè°·æ­Œå®˜æ–¹å‘å¸ƒçš„**è‹±æ–‡BERT-largeï¼ˆwwmï¼‰**æ¨¡å‹ï¼š

*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip)**:
    24-layer, 1024-hidden, 16-heads, 340M parameters

*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip)**:
    24-layer, 1024-hidden, 16-heads, 340M parameters

## å£°æ˜
**è¿™ä¸æ˜¯è°·æ­Œå®˜æ–¹å‘å¸ƒçš„Chinese BERT-base (wwm)ã€‚**

æŠ€æœ¯æŠ¥å‘Šä¸­æ‰€å‘ˆç°çš„å®éªŒç»“æœä»…è¡¨æ˜åœ¨ç‰¹å®šæ•°æ®é›†å’Œè¶…å‚ç»„åˆä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸èƒ½ä»£è¡¨å„ä¸ªæ¨¡å‹çš„æœ¬è´¨ã€‚
å®éªŒç»“æœå¯èƒ½å› éšæœºæ•°ç§å­ï¼Œè®¡ç®—è®¾å¤‡è€Œå‘ç”Ÿæ”¹å˜ã€‚
ç”±äºæˆ‘ä»¬æ²¡æœ‰ç›´æ¥åœ¨PaddlePaddleä¸Šä½¿ç”¨ERNIEï¼Œæ‰€ä»¥åœ¨ERNIEä¸Šçš„å®éªŒç»“æœä»…ä¾›å‚è€ƒï¼ˆè™½ç„¶æˆ‘ä»¬åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå¤ç°äº†æ•ˆæœï¼‰ã€‚

## å¼•ç”¨
å¦‚æœä½ è§‰å¾—æœ¬ç›®å½•ä¸­çš„å†…å®¹å¯¹ç ”ç©¶å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œè¯·åœ¨æ–‡çŒ®ä¸­å¼•ç”¨ä¸‹è¿°æŠ€æœ¯æŠ¥å‘Šï¼š
https://arxiv.org/abs/1906.08101
```
@article{chinese-bert-wwm,
  title={Pre-Training with Whole Word Masking for Chinese BERT},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing and Wang, Shijin and Hu, Guoping},
  journal={arXiv preprint arXiv:1906.08101},
  year={2019}
 }
```

## é—®é¢˜åé¦ˆ
å¦‚æœ‰é—®é¢˜ï¼Œè¯·åœ¨GitHub Issueä¸­æäº¤ã€‚
